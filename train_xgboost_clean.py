#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
XGBoost Training Script for DDoS SDN Detection
Complete version with updated plotting functions
"""

import numpy as np
import pandas as pd
import pickle
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV, RepeatedStratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

def load_and_preprocess_data(file_path):
    """
    Load and preprocess the SDN dataset
    """
    print("üìÇ Loading dataset...")
    train_df = pd.read_csv(file_path)
    print(f"Dataset shape: {train_df.shape}")
    
    # Handle missing values
    print("üîß Handling missing values...")
    train_df["rx_kbps"].fillna(train_df["rx_kbps"].mean(), inplace=True)
    train_df["tot_kbps"].fillna(train_df["tot_kbps"].mean(), inplace=True)
    
    # Remove unnecessary columns (same as notebook)
    remove_cols = ["dt", "tx_kbps", "pktperflow", "pktrate"]
    existing_remove_cols = [col for col in remove_cols if col in train_df.columns]
    if existing_remove_cols:
        train_df.drop(existing_remove_cols, axis=1, inplace=True)
        print(f"Removed columns: {existing_remove_cols}")
    
    # Apply Label Encoding to categorical columns and save encoders
    print("üè∑Ô∏è Applying Label Encoding...")
    label_encoders = {}
    
    # Identify categorical columns
    categorical_cols = ['src', 'dst', 'Protocol', 'Pairflow', 'port_no']
    
    for col in categorical_cols:
        if col in train_df.columns:
            le = LabelEncoder()
            train_df[col] = le.fit_transform(train_df[col])
            label_encoders[col] = le
            print(f"Encoded {col}: {dict(zip(le.classes_, range(len(le.classes_))))}")
    
    print(f"Final dataset shape: {train_df.shape}")
    print(f"Columns: {list(train_df.columns)}")
    return train_df, label_encoders

def prepare_features_target(df):
    """
    Separate features and target variable
    """
    print("üéØ Preparing features and target...")
    X = df.drop(columns='label')
    y = df['label']
    
    # Standardize the features (as in notebook)
    print("üìè Standardizing features...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    print(f"Features shape: {X_scaled.shape}")
    print(f"Target distribution:\n{y.value_counts()}")
    
    return X_scaled, y, scaler

def train_xgboost_model(X_train, y_train):
    """
    Train XGBoost model with RandomizedSearchCV (exact same as notebook)
    """
    print("üöÄ Training XGBoost model with hyperparameter tuning...")
    
    # XGBoost parameters (same as notebook)
    xgb_params = {
        'n_estimators': [100, 200, 300],
        'learning_rate': [0.1],
        'max_depth': [None, 5, 10, 20, 30],
        'min_child_weight': [1],
        'gamma': [0, 0.5, 1, 1.5, 2, 5],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'colsample_bylevel': [1],
        'reg_alpha': [1],
        'reg_lambda': [0],
    }
    
    # Cross-validation setup (same as notebook)
    skf = RepeatedStratifiedKFold(n_splits=3)
    
    # RandomizedSearchCV (same as notebook)
    xgb = RandomizedSearchCV(
        estimator=XGBClassifier(random_state=42),
        param_distributions=xgb_params, 
        cv=skf, 
        n_iter=5, 
        n_jobs=4, 
        verbose=0,
        random_state=42
    )
    
    # Fit model
    xgb_model = xgb.fit(X_train, y_train)
    
    print(f"‚úÖ Best parameters: {xgb_model.best_params_}")
    print(f"‚úÖ Best cross-validation score: {xgb_model.best_score_:.4f}")
    
    return xgb_model

def evaluate_model(model, X_train, X_test, y_train, y_test):
    """
    Evaluate the trained model
    """
    print("üìä Evaluating model...")
    
    # Predictions
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    
    # Calculate metrics
    train_accuracy = accuracy_score(y_train, train_pred)
    test_accuracy = accuracy_score(y_test, test_pred)
    train_precision = precision_score(y_train, train_pred)
    test_precision = precision_score(y_test, test_pred)
    train_recall = recall_score(y_train, train_pred)
    test_recall = recall_score(y_test, test_pred)
    
    # Print results
    print(f"Training Accuracy: {train_accuracy * 100:.2f}%")
    print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
    print(f"Training Precision: {train_precision * 100:.2f}%")
    print(f"Test Precision: {test_precision * 100:.2f}%")
    print(f"Training Recall: {train_recall * 100:.2f}%")
    print(f"Test Recall: {test_recall * 100:.2f}%")
    
    # Classification report
    print("\nüìã Classification Report:")
    print(classification_report(y_test, test_pred))
    
    return {
        'train_accuracy': train_accuracy,
        'test_accuracy': test_accuracy,
        'train_precision': train_precision,
        'test_precision': test_precision,
        'train_recall': train_recall,
        'test_recall': test_recall
    }

def save_model_pickle(model, scaler, feature_columns, metrics, label_encoders, filename='xgb_sdn_model_clean.pkl'):
    """
    Save the trained model and preprocessing components to pickle file
    """
    print(f"üíæ Saving model to {filename}...")
    
    model_package = {
        'model': model,
        'scaler': scaler,
        'feature_columns': feature_columns,
        'metrics': metrics,
        'label_encoders': label_encoders,
        'model_type': 'XGBoost',
        'description': 'DDoS SDN Detection Model'
    }
    
    with open(filename, 'wb') as f:
        pickle.dump(model_package, f)
    
    print(f"‚úÖ Model saved successfully to {filename}")
    return filename

# ==================== PLOTTING FUNCTIONS ====================

def plot_confusion_matrix(y_test, y_pred, save_path='confusion_matrix.png'):
    """
    V·∫Ω bi·ªÉu ƒë·ªì confusion matrix
    """
    print("üìä Plotting confusion matrix...")
    
    # T·∫°o confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    
    # Thi·∫øt l·∫≠p figure
    plt.figure(figsize=(8, 6))
    
    # V·∫Ω heatmap
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                cbar_kws={'label': 'Count'},
                xticklabels=['Benign', 'Malicious'], 
                yticklabels=['Benign', 'Malicious'])
    
    # Thi·∫øt l·∫≠p labels v√† title
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    plt.title('Confusion Matrix - XGBoost Model', fontsize=14, fontweight='bold')
    
    # ƒêi·ªÅu ch·ªânh layout
    plt.tight_layout()
    
    # Hi·ªÉn th·ªã v√† l∆∞u
    plt.savefig(save_path)
    plt.show()

    plt.close()
    
    print(f"‚úÖ Confusion matrix saved to {save_path}")

def plot_classification_report_heatmap(y_test, y_pred, save_path='classification_report_heatmap.png'):
    """
    V·∫Ω bi·ªÉu ƒë·ªì classification report d∆∞·ªõi d·∫°ng heatmap
    """
    print("üìä Plotting classification report heatmap...")
    
    # T·∫°o classification report
    report_dict = classification_report(y_test, y_pred, output_dict=True)
    
    # Chuy·ªÉn ƒë·ªïi th√†nh DataFrame
    df = pd.DataFrame(report_dict).transpose().round(3)
    
    # Lo·∫°i b·ªè h√†ng 'support' v√† c·ªôt 'support' ƒë·ªÉ ch·ªâ gi·ªØ precision, recall, f1-score
    df = df.drop('support', axis=1)
    df = df.drop(['accuracy', 'macro avg', 'weighted avg'], axis=0)
    
    # Thi·∫øt l·∫≠p figure
    plt.figure(figsize=(10, 6))
    
    # V·∫Ω heatmap
    sns.heatmap(df, annot=True, cmap='YlOrRd', fmt=".3f", 
                linewidths=0.5, linecolor='gray',
                cbar_kws={'label': 'Score'})
    
    # Thi·∫øt l·∫≠p labels v√† title
    plt.title("Classification Report Heatmap", fontsize=14, fontweight='bold')
    plt.ylabel("Classes", fontsize=12)
    plt.xlabel("Metrics", fontsize=12)
    
    # ƒêi·ªÅu ch·ªânh layout
    plt.tight_layout()
    
    # Hi·ªÉn th·ªã v√† l∆∞u
    
    plt.savefig(save_path)
    plt.show()

    plt.close()
    
    print(f"‚úÖ Classification report heatmap saved to {save_path}")

def plot_metrics_comparison(metrics_dict, save_path='metrics_comparison.png'):
    """
    V·∫Ω bi·ªÉu ƒë·ªì so s√°nh c√°c metrics gi·ªØa training v√† test
    """
    print("üìä Plotting metrics comparison...")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu
    train_metrics = [
        metrics_dict['train_accuracy'],
        metrics_dict['train_precision'], 
        metrics_dict['train_recall']
    ]
    
    test_metrics = [
        metrics_dict['test_accuracy'],
        metrics_dict['test_precision'],
        metrics_dict['test_recall']
    ]
    
    metric_names = ['Accuracy', 'Precision', 'Recall']
    
    # Thi·∫øt l·∫≠p figure
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # V·ªã tr√≠ bars
    x = np.arange(len(metric_names))
    width = 0.35
    
    # V·∫Ω bars
    bars1 = ax.bar(x - width/2, train_metrics, width, label='Training', 
                   color='skyblue', alpha=0.8)
    bars2 = ax.bar(x + width/2, test_metrics, width, label='Test', 
                   color='lightcoral', alpha=0.8)
    
    # Th√™m gi√° tr·ªã tr√™n bars
    for bar in bars1:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=10)
    
    for bar in bars2:
        height = bar.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=10)
    
    # Thi·∫øt l·∫≠p labels v√† title
    ax.set_xlabel('Metrics', fontsize=12)
    ax.set_ylabel('Score', fontsize=12)
    ax.set_title('Model Performance Comparison: Training vs Test', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(metric_names)
    ax.legend()
    ax.set_ylim(0, 1.1)
    
    # Th√™m grid
    ax.grid(True, alpha=0.3)
    
    # ƒêi·ªÅu ch·ªânh layout
    plt.tight_layout()
    
    # Hi·ªÉn th·ªã v√† l∆∞u
    
    plt.savefig(save_path)
    plt.show()
    plt.close()
    
    print(f"‚úÖ Metrics comparison saved to {save_path}")

def plot_class_distribution(y_train, y_test, save_path='class_distribution.png'):
    """
    V·∫Ω bi·ªÉu ƒë·ªì ph√¢n ph·ªëi c√°c class trong training v√† test set
    """
    print("üìä Plotting class distribution...")
    
    # ƒê·∫øm s·ªë l∆∞·ª£ng m·ªói class
    train_counts = pd.Series(y_train).value_counts().sort_index()
    test_counts = pd.Series(y_test).value_counts().sort_index()
    
    # Thi·∫øt l·∫≠p figure
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Training set distribution
    colors = ['lightblue', 'lightcoral']
    labels = ['Benign', 'Malicious']
    
    ax1.pie(train_counts.values, labels=labels, autopct='%1.1f%%', 
            colors=colors, startangle=90)
    ax1.set_title('Training Set Distribution', fontsize=12, fontweight='bold')
    
    # Test set distribution
    ax2.pie(test_counts.values, labels=labels, autopct='%1.1f%%', 
            colors=colors, startangle=90)
    ax2.set_title('Test Set Distribution', fontsize=12, fontweight='bold')
    
    # ƒêi·ªÅu ch·ªânh layout
    plt.tight_layout()
    
    # Hi·ªÉn th·ªã v√† l∆∞u
    plt.savefig(save_path)
    plt.show()
    plt.close()
    
    print(f"‚úÖ Class distribution saved to {save_path}")

def plot_all_charts(model, X_test, y_test, y_train, metrics_dict):
    """
    V·∫Ω t·∫•t c·∫£ bi·ªÉu ƒë·ªì
    """
    print("üé® Generating all visualization charts...")
    
    # D·ª± ƒëo√°n
    y_pred = model.predict(X_test)
    
    # V·∫Ω t·ª´ng bi·ªÉu ƒë·ªì
    plot_confusion_matrix(y_test, y_pred)
    plot_classification_report_heatmap(y_test, y_pred)
    plot_metrics_comparison(metrics_dict)
    plot_class_distribution(y_train, y_test)
    
    print("‚úÖ All charts generated successfully!")

# ==================== MAIN FUNCTION ====================

def main():
    """
    Main training pipeline
    """
    print("üéØ Starting XGBoost Training Pipeline for DDoS SDN Detection")
    print("=" * 60)
    
    # Load and preprocess data
    df, label_encoders = load_and_preprocess_data('dataset_sdn.csv')
    
    # Prepare features and target
    X, y, scaler = prepare_features_target(df)
    
    # Get feature column names before scaling
    feature_columns = [col for col in df.columns if col != 'label']
    
    # Split data (same as notebook)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    print(f"üìä Train set: {X_train.shape}, Test set: {X_test.shape}")
    
    # Train model
    model = train_xgboost_model(X_train, y_train)
    
    # Evaluate model
    metrics = evaluate_model(model, X_train, X_test, y_train, y_test)
    
    # Save model to pickle
    save_model_pickle(model, scaler, feature_columns, metrics, label_encoders)
    
    # Generate all visualization charts
    plot_all_charts(model, X_test, y_test, y_train, metrics)

    print("\nüéâ Training completed successfully!")
    print("=" * 60)

if __name__ == "__main__":
    main()