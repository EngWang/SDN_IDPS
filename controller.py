from ryu.base import app_manager
from ryu.controller import ofp_event
from ryu.controller.handler import MAIN_DISPATCHER, CONFIG_DISPATCHER, DEAD_DISPATCHER
from ryu.controller.handler import set_ev_cls
from ryu.ofproto import ofproto_v1_3
from ryu.lib.packet import packet, ethernet, ipv4, tcp, udp, ether_types
import pickle
import numpy as np
import time
from ryu.lib import hub
from operator import attrgetter
import pandas as pd

IDLE_TIMEOUT_FOR_STATS = 2 
DEFAULT_FLOW_PRIORITY = 1
ATTACK_BLOCK_PRIORITY = 10
MODEL_PATH = r'D:\m√¥n h·ªçc\DACN\beta\xgb_sdn_model_clean.pkl'

MONITORING_INTERVAL = 30.0 
MAX_SWITCHES = 10  
MAX_PORTS_PER_SWITCH = 5 

# --- Ph√°t hi·ªán t·∫•n c√¥ng ---
DETECTION_MIN_PACKETS = 10  # S·ªë g√≥i tin t·ªëi thi·ªÉu
DETECTION_MIN_DURATION = 1.0  # Th·ªùi gian t·ªëi thi·ªÉu
DETECTION_INTERVAL = 5.0  # Kho·∫£ng th·ªùi gian gi·ªØa c√°c l·∫ßn

class SDN_IDPS(app_manager.RyuApp):
    OFP_VERSIONS = [ofproto_v1_3.OFP_VERSION]

    def __init__(self, *args, **kwargs):
        super(SDN_IDPS, self).__init__(*args, **kwargs)
        self.mac_to_port = {}
        self.flows = {} # Key:(src_ip, dst_ip, protocol).
        self.datapaths = {}
        self.monitor_thread = hub.spawn(self._monitor)
        self.active_flows_per_switch = {}  # {datapath_id: count}
        self.last_analysis_time = {}  # {flow_key: timestamp} ƒë·ªÉ tr√°nh ph√¢n t√≠ch qu√° th∆∞·ªùng xuy√™n
        self.early_detection_stats = {'analyzed': 0, 'detected': 0, 'blocked': 0}  # Th·ªëng k√™ ph√°t hi·ªán s·ªõm

        self.model = None
        self.scaler = None
        self.feature_columns = None
        self.model_metrics = None
        self.label_encoders = None
        try:
            with open(MODEL_PATH, 'rb') as f:
                model_package = pickle.load(f)
            self.model = model_package['model']
            self.scaler = model_package['scaler']
            self.feature_columns = model_package['feature_columns']
            self.model_metrics = model_package['metrics']
            self.label_encoders = model_package.get('label_encoders', {})            
            self.logger.info("‚úÖ Load m√¥ h√¨nh XGBoost th√†nh c√¥ng!")
        except Exception:
            self.logger.error(f"‚ùå Kh√¥ng th·ªÉ load m√¥ h√¨nh!")

    def _monitor(self):
        while True:
            for dp in list(self.datapaths.values()):
                self._request_stats(dp)
            
            # Ph√¢n t√≠ch real-time c√°c flow ƒëang ho·∫°t ƒë·ªông ƒë·ªÉ ph√°t hi·ªán t·∫•n c√¥ng s·ªõm
            self._analyze_active_flows()
            
            hub.sleep(7)

    def _request_stats(self, datapath):
        # G·ª≠i y√™u c·∫ßu th·ªëng k√™ chung.
        self.logger.debug('G·ª≠i y√™u c·∫ßu th·ªëng k√™ ƒë·∫øn switch: %016x', datapath.id)
        parser = datapath.ofproto_parser
        req_flow = parser.OFPFlowStatsRequest(datapath)
        datapath.send_msg(req_flow)
        req_port = parser.OFPPortStatsRequest(datapath, 0, datapath.ofproto.OFPP_ANY)
        datapath.send_msg(req_port)

    def _analyze_active_flows(self):
        """
        Ph√¢n t√≠ch real-time c√°c flow ƒëang ho·∫°t ƒë·ªông ƒë·ªÉ ph√°t hi·ªán t·∫•n c√¥ng s·ªõm
        """
        if not self.model or not self.scaler:
            return

        current_time = time.time()
        flows_to_analyze = []

        # T√¨m c√°c flow c√≥ ƒë·ªß ƒëi·ªÅu ki·ªán ƒë·ªÉ ph√¢n t√≠ch s·ªõm
        for flow_key, flow_data in self.flows.items():
            # Ki·ªÉm tra ƒëi·ªÅu ki·ªán ph√¢n t√≠ch s·ªõm
            if self._should_analyze_flow_early(flow_key, flow_data, current_time):
                flows_to_analyze.append((flow_key, flow_data))

        # Ph√¢n t√≠ch c√°c flow ƒë∆∞·ª£c ch·ªçn
        for flow_key, flow_data in flows_to_analyze:
            self._analyze_single_flow_early(flow_key, flow_data, current_time)

    def _should_analyze_flow_early(self, flow_key, flow_data, current_time):
        """
        Ki·ªÉm tra xem flow c√≥ n√™n ƒë∆∞·ª£c ph√¢n t√≠ch s·ªõm hay kh√¥ng
        """
        # Ki·ªÉm tra s·ªë g√≥i tin t·ªëi thi·ªÉu
        if flow_data['packet_count'] < DETECTION_MIN_PACKETS:
            return False

        # Ki·ªÉm tra th·ªùi gian t·ªëi thi·ªÉu
        duration = current_time - flow_data['start_time']
        if duration < DETECTION_MIN_DURATION:
            return False

        # Ki·ªÉm tra kho·∫£ng th·ªùi gian gi·ªØa c√°c l·∫ßn ph√¢n t√≠ch
        last_analysis = self.last_analysis_time.get(flow_key, 0)
        if current_time - last_analysis < DETECTION_INTERVAL:
            return False

        # Ki·ªÉm tra flow ch∆∞a b·ªã ch·∫∑n
        if flow_data.get('is_blocked', False):
            return False

        return True

    def _analyze_single_flow_early(self, flow_key, flow_data, current_time):
        """
        Ph√¢n t√≠ch m·ªôt flow c·ª• th·ªÉ ƒë·ªÉ ph√°t hi·ªán t·∫•n c√¥ng s·ªõm
        """
        try:
            # C·∫≠p nh·∫≠t th·ªùi gian ph√¢n t√≠ch cu·ªëi
            self.last_analysis_time[flow_key] = current_time

            # T√≠nh s·ªë flow hi·ªán t·∫°i tr√™n switch (∆∞·ªõc t√≠nh)
            flows_count = len(self.flows)

            # Tr√≠ch xu·∫•t features
            features_df = self.extract_features(flow_data, flows_count=flows_count)
            
            # Ph√¢n lo·∫°i
            features_scaled = self.scaler.transform(features_df)
            prediction_probabilities = self.model.predict_proba(features_scaled)[0]
            predicted_class = self.model.predict(features_scaled)[0]
            malicious_probability = prediction_probabilities[1] if len(prediction_probabilities) > 1 else prediction_probabilities[0]
            confidence_score = malicious_probability * 100

            predicted_class_name = 'MALICIOUS' if predicted_class == 1 else 'BENIGN'

            # C·∫≠p nh·∫≠t th·ªëng k√™
            self.early_detection_stats['analyzed'] += 1
            
            # Hi·ªÉn th·ªã flow_key v·ªõi protocol l√† t√™n giao th·ª©c
            src_ip, dst_ip, protocol = flow_key
            protocol_name = {6: 'TCP', 17: 'UDP', 1: 'ICMP'}.get(protocol, f'PROTO_{protocol}')
            flow_key_str = (src_ip, dst_ip, protocol_name)

            # Log k·∫øt qu·∫£ ph√¢n t√≠ch s·ªõm
            self.logger.info(f"üîç Flow {flow_key_str} - Class: {predicted_class_name}, Confidence: {confidence_score:.2f}%")

            # N·∫øu ph√°t hi·ªán t·∫•n c√¥ng v·ªõi ƒë·ªô tin c·∫≠y cao
            if predicted_class == 1 and confidence_score > 70:  # Ng∆∞·ª°ng tin c·∫≠y 70%
                self.early_detection_stats['detected'] += 1
                self.logger.warning(f"üö® DETECTION: Ph√°t hi·ªán t·∫•n c√¥ng s·ªõm trong flow {flow_key_str}! Confidence: {confidence_score:.2f}%")
                
                # ƒê√°nh d·∫•u flow ƒë√£ b·ªã ch·∫∑n
                flow_data['is_blocked'] = True
                
                # T√¨m datapath ƒë·ªÉ c√†i rule ch·∫∑n
                if self._block_flow_early(flow_key, flow_data):
                    self.early_detection_stats['blocked'] += 1

        except Exception as e:
            self.logger.error(f"‚ùå L·ªói khi ph√¢n t√≠ch flow {flow_key}: {str(e)}")

    def _block_flow_early(self, flow_key, flow_data):
        """
        Ch·∫∑n flow s·ªõm khi ph√°t hi·ªán t·∫•n c√¥ng
        """
        try:
            src_ip, dst_ip, protocol = flow_key
            protocol_name = {6: 'TCP', 17: 'UDP', 1: 'ICMP'}.get(protocol, f'PROTO_{protocol}')
            flow_key_str = (src_ip, dst_ip, protocol_name)
            
            # T√¨m datapath ph√π h·ª£p d·ª±a tr√™n in_port c·ªßa flow
            target_datapath = None
            in_port = flow_data.get('in_port')
            
            if in_port and self.datapaths:
                for datapath in self.datapaths.values():
                    target_datapath = datapath
                    break
            if not target_datapath and self.datapaths:
                target_datapath = list(self.datapaths.values())[0]
            
            if target_datapath:
                parser = target_datapath.ofproto_parser
                match_fields = {
                    'eth_type': ether_types.ETH_TYPE_IP,
                    'ipv4_src': src_ip,
                }
                block_match = parser.OFPMatch(**match_fields)
                self.add_flow(target_datapath, ATTACK_BLOCK_PRIORITY, block_match, [], hard_timeout=3600)
                self.logger.info(f"üö´ ƒê√£ c√†i rule ch·∫∑n cho flow {flow_key_str} tr√™n datapath {target_datapath.id}")
                return True
            else:
                self.logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y datapath ƒë·ªÉ ch·∫∑n flow {flow_key_str}")
                return False
        except Exception as e:
            self.logger.error(f"‚ùå L·ªói khi ch·∫∑n flow {flow_key}: {str(e)}")
            return False

    @set_ev_cls(ofp_event.EventOFPStateChange, [MAIN_DISPATCHER, DEAD_DISPATCHER])
    def _state_change_handler(self, ev):
        datapath = ev.datapath
        if ev.state == MAIN_DISPATCHER:
            if datapath.id not in self.datapaths:
                self.logger.debug('ƒêƒÉng k√Ω switch: %016x', datapath.id)
                self.datapaths[datapath.id] = datapath
        elif ev.state == DEAD_DISPATCHER:
            if datapath.id in self.datapaths:
                self.logger.debug('H·ªßy ƒëƒÉng k√Ω switch: %016x', datapath.id)
                if datapath.id in self.datapaths:
                    del self.datapaths[datapath.id]

    @set_ev_cls(ofp_event.EventOFPFlowStatsReply, MAIN_DISPATCHER)
    def _flow_stats_reply_handler(self, ev):
        body = ev.msg.body

        # Gom nh√≥m flow theo src_ip, dst_ip, protocol
        flow_groups = {}
        
        for stat in [flow for flow in body if flow.priority == 1]:
            # L·∫•y th√¥ng tin IP t·ª´ match fields
            src_ip = stat.match.get('ipv4_src')
            dst_ip = stat.match.get('ipv4_dst')
            protocol = stat.match.get('ip_proto')
            
            # Ch·ªâ x·ª≠ l√Ω flow c√≥ th√¥ng tin IP
            if src_ip and dst_ip and protocol is not None:
                # T·∫°o key nh√≥m
                group_key = (src_ip, dst_ip, protocol)
                
                if group_key not in flow_groups:
                    flow_groups[group_key] = {
                        'src_ip': src_ip,
                        'dst_ip': dst_ip,
                        'protocol': protocol,
                        'total_packets': 0,
                        'total_bytes': 0,
                        'flow_count': 0,
                        'ports': set()  # T·∫≠p h·ª£p c√°c port ƒë∆∞·ª£c s·ª≠ d·ª•ng
                    }
                
                # C·ªông d·ªìn th·ªëng k√™
                flow_groups[group_key]['total_packets'] += stat.packet_count
                flow_groups[group_key]['total_bytes'] += stat.byte_count
                flow_groups[group_key]['flow_count'] += 1
                
                # Th√™m port v√†o t·∫≠p h·ª£p
                in_port = stat.match.get('in_port')
                if in_port is not None:
                    flow_groups[group_key]['ports'].add(in_port)
        
        # Hi·ªÉn th·ªã th·ªëng k√™ gom nh√≥m
        if flow_groups:
            self.logger.info('=== FLOW STATISTICS (GROUPED BY SRC/DST IP) ===')
            self.logger.info('datapath         src_ip          dst_ip          protocol  flows  packets    bytes    ports')
            self.logger.info('---------------- --------------- --------------- --------  -----  --------  --------  -----')
            
            # S·∫Øp x·∫øp theo t·ªïng s·ªë packets (gi·∫£m d·∫ßn)
            sorted_groups = sorted(flow_groups.items(), 
                                 key=lambda x: x[1]['total_packets'], reverse=True)
            
            for group_key, group_data in sorted_groups:
                src_ip, dst_ip, protocol = group_key
                protocol_name = {6: 'TCP', 17: 'UDP', 1: 'ICMP'}.get(protocol, f'PROTO{protocol}')
                ports_str = ','.join(map(str, sorted(group_data['ports']))) if group_data['ports'] else 'N/A'
                
                self.logger.info('%016x %15s %15s %8s %6d %8d %8d  %s',
                                ev.msg.datapath.id,
                                src_ip, dst_ip, protocol_name,
                                group_data['flow_count'],
                                group_data['total_packets'],
                                group_data['total_bytes'],
                                ports_str)
            
            # Th·ªëng k√™ t·ªïng quan
            total_groups = len(flow_groups)
            total_flows = sum(g['flow_count'] for g in flow_groups.values())
            total_packets = sum(g['total_packets'] for g in flow_groups.values())
            total_bytes = sum(g['total_bytes'] for g in flow_groups.values())
            
            self.logger.info('=== SUMMARY ===')
            self.logger.info('Total IP groups: %d, Total flows: %d, Total packets: %d, Total bytes: %d',
                            total_groups, total_flows, total_packets, total_bytes)
        else:
            self.logger.info('No IP flows found in flow statistics')

    @set_ev_cls(ofp_event.EventOFPPortStatsReply, MAIN_DISPATCHER)
    def _port_stats_reply_handler(self, ev):
        body = ev.msg.body

        self.logger.info('datapath         port     '
                        'rx-pkts    rx-bytes   '
                        'tx-pkts    tx-bytes   ')
        self.logger.info('---------------- -------- '
                        '---------- ---------- '
                        '---------- ---------- ')
        for stat in sorted(body, key=attrgetter('port_no')):
            self.logger.info('%016x %8x %10d %10d %10d %10d',
                            ev.msg.datapath.id, stat.port_no,
                            stat.rx_packets, stat.rx_bytes,
                            stat.tx_packets, stat.tx_bytes)

    @set_ev_cls(ofp_event.EventOFPSwitchFeatures, CONFIG_DISPATCHER)
    def switch_features_handler(self, ev):
        datapath = ev.msg.datapath
        ofproto = datapath.ofproto
        parser = datapath.ofproto_parser

        match = parser.OFPMatch()
        actions = [parser.OFPActionOutput(ofproto.OFPP_CONTROLLER, ofproto.OFPCML_NO_BUFFER)]
        self.add_flow(datapath, 0, match, actions)

    def add_flow(self, datapath, priority, match, actions, buffer_id=None, idle_timeout=0, hard_timeout=0, flags=0):
        # H√†m ti·ªán √≠ch th√™m flow rule. C√°c tham s·ªë idle_timeout v√† flags r·∫•t quan tr·ªçng cho IDPS.
        ofproto = datapath.ofproto
        parser = datapath.ofproto_parser
        inst = [parser.OFPInstructionActions(ofproto.OFPIT_APPLY_ACTIONS, actions)]
        mod_args = {
            'datapath': datapath, 'priority': priority, 'match': match,
            'instructions': inst, 'idle_timeout': idle_timeout, # Timeout ƒë·ªÉ k√≠ch ho·∫°t FlowRemoved.
            'hard_timeout': hard_timeout, 'flags': flags # C·ªù OFPFF_SEND_FLOW_REM ƒë·ªÉ nh·∫≠n s·ª± ki·ªán.
        }
        if buffer_id and buffer_id != ofproto.OFP_NO_BUFFER:
             mod_args['buffer_id'] = buffer_id
        mod = parser.OFPFlowMod(**mod_args)
        datapath.send_msg(mod)

    @set_ev_cls(ofp_event.EventOFPPacketIn, MAIN_DISPATCHER)
    def _packet_in_handler(self, ev):
        # Ki·ªÉm tra model v√† scaler tr∆∞·ªõc khi x·ª≠ l√Ω cho IDPS.
        if not self.model or not self.scaler:
            self.logger.warning("IDPS: Model/Scaler ch∆∞a t·∫£i, b·ªè qua x·ª≠ l√Ω PacketIn cho IDPS.")
            return

        msg = ev.msg
        datapath = msg.datapath
        ofproto = datapath.ofproto
        parser = datapath.ofproto_parser
        in_port = msg.match['in_port']

        pkt = packet.Packet(msg.data)
        eth = pkt.get_protocol(ethernet.ethernet)
        if not eth or eth.ethertype == ether_types.ETH_TYPE_LLDP:
            return

        dst_mac = eth.dst
        src_mac = eth.src
        dpid = format(datapath.id, "d").zfill(16)
        self.mac_to_port.setdefault(dpid, {})
        self.mac_to_port[dpid][src_mac] = in_port
        out_port = self.mac_to_port[dpid].get(dst_mac, ofproto.OFPP_FLOOD)
        actions_packet_out = [parser.OFPActionOutput(out_port)]

        ipv4_pkt = pkt.get_protocol(ipv4.ipv4)
        if ipv4_pkt: 
            src_ip = ipv4_pkt.src
            dst_ip = ipv4_pkt.dst
            protocol = ipv4_pkt.proto
            packet_length = msg.total_len

            tcp_pkt = pkt.get_protocol(tcp.tcp)
            udp_pkt = pkt.get_protocol(udp.udp)
            src_port = tcp_pkt.src_port if tcp_pkt else (udp_pkt.src_port if udp_pkt else 0)
            dst_port = tcp_pkt.dst_port if tcp_pkt else (udp_pkt.dst_port if udp_pkt else 0)

            if protocol in [6, 17, 1]:  # TCP, UDP, ICMP
                flow_key = self.get_flow_key(src_ip, dst_ip, protocol)
                now = time.time()
                self.update_flow(flow_key, now, packet_length, tcp_pkt, ipv4_pkt, in_port)

                # C·∫≠p nh·∫≠t s·ªë l∆∞·ª£ng flow ƒëang ho·∫°t ƒë·ªông tr√™n switch
                dpid_num = datapath.id
                self.active_flows_per_switch.setdefault(dpid_num, 0)
                self.active_flows_per_switch[dpid_num] += 1

                # C√†i ƒë·∫∑t flow rule t·∫°m th·ªùi ƒë·ªÉ thu th·∫≠p th·ªëng k√™ v√† k√≠ch ho·∫°t FlowRemoved.
                match_fields_dict = {'eth_type': ether_types.ETH_TYPE_IP, 'ipv4_src': src_ip, 'ipv4_dst': dst_ip, 'ip_proto': protocol}
                if tcp_pkt: match_fields_dict.update({'tcp_src': src_port, 'tcp_dst': dst_port})
                elif udp_pkt: match_fields_dict.update({'udp_src': src_port, 'udp_dst': dst_port})
                match_for_flow_rule = parser.OFPMatch(**match_fields_dict)

                # Th√™m flow rule v·ªõi idle_timeout v√† c·ªù OFPFF_SEND_FLOW_REM.
                self.add_flow(datapath, DEFAULT_FLOW_PRIORITY, match_for_flow_rule, actions_packet_out,
                                msg.buffer_id if msg.buffer_id != ofproto.OFP_NO_BUFFER else None,
                                idle_timeout=IDLE_TIMEOUT_FOR_STATS, # Quan tr·ªçng cho vi·ªác k√≠ch ho·∫°t FlowRemoved.
                                flags=ofproto.OFPFF_SEND_FLOW_REM)   # Quan tr·ªçng ƒë·ªÉ nh·∫≠n s·ª± ki·ªán.
        
        # G·ª≠i g√≥i tin PacketIn hi·ªán t·∫°i ra ngo√†i.
        data_to_send = msg.data if msg.buffer_id == ofproto.OFP_NO_BUFFER else None
        out_message = parser.OFPPacketOut(datapath=datapath, buffer_id=msg.buffer_id,
                                          in_port=in_port, actions=actions_packet_out, data=data_to_send)
        datapath.send_msg(out_message)

    @set_ev_cls(ofp_event.EventOFPFlowRemoved, MAIN_DISPATCHER)
    def _flow_removed_handler(self, ev):
        """
        Khi x√≥a flow, ghi log th√¥ng tin flow b·ªã x√≥a.
        """
        msg = ev.msg
        datapath = msg.datapath
        match_fields = msg.match
        src_ip = match_fields.get('ipv4_src')
        dst_ip = match_fields.get('ipv4_dst')
        protocol = match_fields.get('ip_proto')
        if not (src_ip and dst_ip and protocol is not None):
            return

        flow_key_reconstructed = self.get_flow_key(src_ip, dst_ip, protocol)
        if flow_key_reconstructed in self.flows:
            del self.flows[flow_key_reconstructed]
        dpid_num = datapath.id
        if dpid_num in self.active_flows_per_switch and self.active_flows_per_switch[dpid_num] > 0:
            self.active_flows_per_switch[dpid_num] -= 1

    def get_flow_key(self, src_ip, dst_ip, protocol):
        # T·∫°o key ch·ªâ d·ª±a tr√™n src_ip, dst_ip, protocol ƒë·ªÉ gom c√°c flow theo ƒë√∫ng dataset
        return (src_ip, dst_ip, protocol)

    def get_reverse_flow_key(self, src_ip, dst_ip, protocol):
        # T·∫°o key cho flow ng∆∞·ª£c l·∫°i
        return (dst_ip, src_ip, protocol)

    def check_pairflow(self, flow_key):
        """
        Ki·ªÉm tra xem c√≥ paired flow (bidirectional) hay kh√¥ng
        Returns: 1 n·∫øu c√≥ paired flow, 0 n·∫øu kh√¥ng
        """
        src_ip, dst_ip, protocol = flow_key
        reverse_key = self.get_reverse_flow_key(src_ip, dst_ip, protocol)
        return 1 if reverse_key in self.flows else 0

    def update_flow(self, flow_key, now, packet_length, tcp_pkt, ipv4_pkt, in_port=None):
        """
        C·∫≠p nh·∫≠t th√¥ng tin th·ªëng k√™ cho flow theo chu·∫©n dataset SDN.
        L∆∞u tr·ªØ ƒë·∫ßy ƒë·ªß th√¥ng tin ƒë·ªÉ t√≠nh to√°n 23 features.
        """
        # L·∫•y th√¥ng tin port t·ª´ flow_key ƒë·ªÉ l∆∞u v√†o flow_stats
        src_ip, dst_ip, protocol_info = flow_key
        
        flow_stats = self.flows.setdefault(flow_key, {
            'start_time': now, 'last_time': now,
            'packet_times': [],
            'packet_lengths': [],
            'packet_count': 0,
            'total_bytes': 0,
            # Th√¥ng tin c∆° b·∫£n cho features
            'src_ip': src_ip,
            'dst_ip': dst_ip,
            'protocol': protocol_info,
            'in_port': in_port,
            'is_blocked': False,
            # Th√¥ng tin b·ªï sung cho t√≠nh to√°n ch√≠nh x√°c
            'first_packet_time': now,
            'last_packet_time': now,
            'tx_bytes': 0,  # Bytes truy·ªÅn
            'rx_bytes': 0,  # Bytes nh·∫≠n
            'packet_ins_count': 0,  # S·ªë l∆∞·ª£ng Packet_in messages
        })

        flow_stats['last_time'] = now
        flow_stats['last_packet_time'] = now
        flow_stats['packet_times'].append(now)
        flow_stats['packet_lengths'].append(packet_length)
        flow_stats['packet_count'] += 1
        flow_stats['total_bytes'] += packet_length
        flow_stats['packet_ins_count'] += 1  # M·ªói packet t·∫°o ra m·ªôt Packet_in

        # T√≠nh to√°n tx_bytes v√† rx_bytes (ƒë∆°n gi·∫£n: chia ƒë√¥i)
        flow_stats['tx_bytes'] = flow_stats['total_bytes'] // 2
        flow_stats['rx_bytes'] = flow_stats['total_bytes'] - flow_stats['tx_bytes'] 

    def extract_features(self, flow_data_dict, flows_count=1, datapath_id=None):
        """
        Tr√≠ch xu·∫•t features t·ª´ flow data theo ƒë√∫ng format c·ªßa dataset SDN
        D·ª±a tr√™n m√¥ t·∫£: 23 features ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ switches v√† t√≠nh to√°n
        
        EXTRACTED FEATURES (t·ª´ switches):
        - Switch-id, Packet_count, byte_count, duration_sec, duration_nsec
        - Source IP, Destination IP, Port number, tx_bytes, rx_bytes, dt
        
        CALCULATED FEATURES (t√≠nh to√°n):
        - Packet per flow, Byte per flow, Packet Rate, Packet_ins
        - Total flow entries, tx_kbps, rx_kbps, Port Bandwidth
        - Pairflow, Protocol, Total kbps
        """
        # --- T√≠nh to√°n c√°c gi√° tr·ªã c∆° b·∫£n ---
        duration = flow_data_dict['last_time'] - flow_data_dict['start_time']
        if duration <= 0: duration = 1e-6 # Tr√°nh chia cho 0.

        # L·∫•y d·ªØ li·ªáu t·ª´ flow_data_dict
        packet_lengths_array = np.array(flow_data_dict['packet_lengths']) if flow_data_dict['packet_lengths'] else np.array([0.0])

        # --- EXTRACTED FEATURES (t·ª´ switches) ---
        
        # 1. Switch-id: ID c·ªßa switch
        if datapath_id is not None:
            switch_id = datapath_id % MAX_SWITCHES  # ƒê·∫£m b·∫£o trong ph·∫°m vi 0-9
        else:
            switch_id = 0  # Default switch ID
        
        # 2. Packet_count: S·ªë l∆∞·ª£ng g√≥i tin trong flow
        pktcount = flow_data_dict['packet_count']
        
        # 3. Byte_count: T·ªïng s·ªë bytes trong flow
        bytecount = flow_data_dict['total_bytes']
        
        # 4. Duration_sec: Th·ªùi gian flow (gi√¢y)
        dur_sec = int(duration)
        
        # 5. Duration_nsec: Th·ªùi gian flow (nano gi√¢y)
        dur_nsec = int(duration * 1_000_000_000)
        
        # 6. Total_duration: T·ªïng th·ªùi gian (nano gi√¢y)
        tot_dur = dur_nsec
        
        # 7. Source IP: IP ngu·ªìn (encoded)
        src_ip = flow_data_dict.get('src_ip', '10.0.0.1')
        if self.label_encoders and 'src' in self.label_encoders:
            try:
                src = self.label_encoders['src'].transform([src_ip])[0]
            except ValueError:
                src = 0
        else:
            try:
                src_last_octet = int(src_ip.split('.')[-1]) if src_ip else 1
                src = max(0, min(18, src_last_octet - 1))
            except:
                src = 0
        
        # 8. Destination IP: IP ƒë√≠ch (encoded)
        dst_ip = flow_data_dict.get('dst_ip', '10.0.0.8')
        if self.label_encoders and 'dst' in self.label_encoders:
            try:
                dst = self.label_encoders['dst'].transform([dst_ip])[0]
            except ValueError:
                dst = 16
        else:
            try:
                dst_last_octet = int(dst_ip.split('.')[-1]) if dst_ip else 8
                dst = max(0, min(17, dst_last_octet - 1))
            except:
                dst = 16
        
        # 9. Port number: S·ªë hi·ªáu c·ªïng switch
        port_no_raw = flow_data_dict.get('in_port', 1)
        if self.label_encoders and 'port_no' in self.label_encoders:
            try:
                port_no = self.label_encoders['port_no'].transform([port_no_raw])[0]
            except ValueError:
                port_no = 0
        else:
            port_no = (port_no_raw - 1) % 5
        
        # 10. tx_bytes: Bytes truy·ªÅn t·ª´ switch port
        tx_bytes = flow_data_dict.get('tx_bytes', bytecount // 2)
        
        # 11. rx_bytes: Bytes nh·∫≠n tr√™n switch port
        rx_bytes = flow_data_dict.get('rx_bytes', bytecount - tx_bytes)
        
        # 12. dt: Date and time (converted to number) - s·ª≠ d·ª•ng timestamp
        dt = int(flow_data_dict['start_time'])
        
        # --- CALCULATED FEATURES (t√≠nh to√°n) ---
        
        # 13. Packet per flow: S·ªë g√≥i tin trong m·ªôt flow
        packetperflow = pktcount
        
        # 14. Byte per flow: S·ªë bytes trong m·ªôt flow
        byteperflow = bytecount
        
        # 15. Packet Rate: S·ªë g√≥i tin/gi√¢y (monitoring interval = 30s)
        # Theo m√¥ t·∫£: "Packet Rate is number of packets send per second and calculated by dividing the packet per flow by monitoring interval"
        packet_rate = pktcount / MONITORING_INTERVAL if MONITORING_INTERVAL > 0 else 0
        
        # 16. Packet_ins: S·ªë l∆∞·ª£ng Packet_in messages
        packetins = flow_data_dict.get('packet_ins_count', pktcount)  # S·ª≠ d·ª•ng gi√° tr·ªã ƒë√£ l∆∞u tr·ªØ
        
        # 17. Total flow entries: T·ªïng s·ªë flow entries trong switch
        flows = flows_count
        
        # 18. tx_kbps: T·ªëc ƒë·ªô truy·ªÅn d·ªØ li·ªáu (Kbps)
        tx_kbps = (tx_bytes * 8 / 1000) / duration if duration > 0 else 0
        
        # 19. rx_kbps: T·ªëc ƒë·ªô nh·∫≠n d·ªØ li·ªáu (Kbps)
        rx_kbps = (rx_bytes * 8 / 1000) / duration if duration > 0 else 0
        
        # 20. Port Bandwidth: T·ªïng bƒÉng th√¥ng c·ªïng (tx_kbps + rx_kbps)
        port_bandwidth = tx_kbps + rx_kbps
        
        # 21. Pairflow: Ki·ªÉm tra c√≥ flow ng∆∞·ª£c l·∫°i kh√¥ng
        flow_key = (flow_data_dict.get('src_ip', ''), flow_data_dict.get('dst_ip', ''), flow_data_dict.get('protocol', 0))
        Pairflow = self.check_pairflow(flow_key)
        
        # 22. Protocol: Lo·∫°i giao th·ª©c (encoded)
        protocol_raw = flow_data_dict.get('protocol', 17)
        if protocol_raw == 6:      # TCP
            protocol_name = 'TCP'
        elif protocol_raw == 17:   # UDP  
            protocol_name = 'UDP'
        elif protocol_raw == 1:    # ICMP
            protocol_name = 'ICMP'
        else:
            protocol_name = 'UDP'
        
        if self.label_encoders and 'Protocol' in self.label_encoders:
            try:
                Protocol = self.label_encoders['Protocol'].transform([protocol_name])[0]
            except ValueError:
                Protocol = 0
        else:
            if protocol_name == 'TCP':
                Protocol = 1
            elif protocol_name == 'UDP':
                Protocol = 2
            elif protocol_name == 'ICMP':
                Protocol = 0
            else:
                Protocol = 2
        
        # 23. Total kbps: T·ªïng t·ªëc ƒë·ªô truy·ªÅn nh·∫≠n
        tot_kbps = (bytecount * 8 / 1000) / duration if duration > 0 else 0

        # --- T·∫°o DataFrame v·ªõi ƒë√∫ng th·ª© t·ª± features t·ª´ model ---
        if self.feature_columns:
            # S·ª≠ d·ª•ng th·ª© t·ª± features t·ª´ model ƒë√£ train
            feature_values = []
            for feature in self.feature_columns:
                if feature == 'switch':
                    feature_values.append(switch_id)
                elif feature == 'src':
                    feature_values.append(src)
                elif feature == 'dst':
                    feature_values.append(dst)
                elif feature == 'pktcount':
                    feature_values.append(pktcount)
                elif feature == 'bytecount':
                    feature_values.append(bytecount)
                elif feature == 'dur':
                    feature_values.append(dur_sec)
                elif feature == 'dur_nsec':
                    feature_values.append(dur_nsec)
                elif feature == 'tot_dur':
                    feature_values.append(tot_dur)
                elif feature == 'flows':
                    feature_values.append(flows)
                elif feature == 'packetins':
                    feature_values.append(packetins)
                elif feature == 'byteperflow':
                    feature_values.append(byteperflow)
                elif feature == 'Pairflow':
                    feature_values.append(Pairflow)
                elif feature == 'Protocol':
                    feature_values.append(Protocol)
                elif feature == 'port_no':
                    feature_values.append(port_no)
                elif feature == 'tx_bytes':
                    feature_values.append(tx_bytes)
                elif feature == 'rx_bytes':
                    feature_values.append(rx_bytes)
                elif feature == 'rx_kbps':
                    feature_values.append(rx_kbps)
                elif feature == 'tot_kbps':
                    feature_values.append(tot_kbps)
                elif feature == 'dt':
                    feature_values.append(dt)
                elif feature == 'packetperflow':
                    feature_values.append(packetperflow)
                elif feature == 'packet_rate':
                    feature_values.append(packet_rate)
                elif feature == 'tx_kbps':
                    feature_values.append(tx_kbps)
                elif feature == 'port_bandwidth':
                    feature_values.append(port_bandwidth)
                else:
                    feature_values.append(0)  # Default value for unknown features
            
            features_df = pd.DataFrame([feature_values], columns=self.feature_columns)
        else:
            # Fallback n·∫øu kh√¥ng c√≥ feature_columns - s·ª≠ d·ª•ng th·ª© t·ª± theo m√¥ t·∫£ dataset
            self.logger.warning("IDPS: Kh√¥ng c√≥ feature_columns t·ª´ model, s·ª≠ d·ª•ng th·ª© t·ª± theo m√¥ t·∫£ dataset")
            feature_names = [
                'switch', 'src', 'dst', 'pktcount', 'bytecount', 'dur', 'dur_nsec', 'tot_dur',
                'packetperflow', 'byteperflow', 'packet_rate', 'packetins', 'flows', 'tx_kbps', 
                'rx_kbps', 'port_bandwidth', 'Pairflow', 'Protocol', 'port_no', 'tx_bytes', 
                'rx_bytes', 'dt', 'tot_kbps'
            ]
            features_list = [
                switch_id, src, dst, pktcount, bytecount, dur_sec, dur_nsec, tot_dur,
                packetperflow, byteperflow, packet_rate, packetins, flows, tx_kbps,
                rx_kbps, port_bandwidth, Pairflow, Protocol, port_no, tx_bytes,
                rx_bytes, dt, tot_kbps
            ]
            features_df = pd.DataFrame([features_list], columns=feature_names)
        
        return features_df   